---
title: Really Good Papers (part 1)
layout: post
---

I've started a challenge to read a machine learning paper a day. Now after a few of months and a lot of catching up to do I've gone through a couple hundred papers. These are my picks for the best ones I found:

(too many to list in the blurb, see [full post]({% post_url 2019-01-29-papers-1 %}))

<img src="/assets/images/paper-stack-1.jpg" width="360" alt="Stack of read papers."/>


<!--more-->

1. The Shattered Gradients Problem: If resnets are the answer, then what is the question? David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, Brian McWilliams [(Balduzzi et al., 2017)]
2. Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning. Jakob N. Foerster, Francis Song, Edward Hughes, Neil Burch, Iain Dunning, Shimon Whiteson, Matthew Botvinick, Michael Bowling [(Foerester et al., 2018)]
3. Modular Networks: Learning to Decompose Neural Computation. Louis Kirsch, Julius Kunze, David Barber [(Kirsch et al., 2018)]
4. Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin [(Vaswani et al., 2017)]
5. Intrinsic Social Motivation via Causal Influence in Multi-Agent RL. Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro A. Ortega, DJ Strouse, Joel Z. Leibo, Nando de Freitas [(Jaques et al., 2018)]
6. Episodic Curiosity through Reachability. Nikolay Savinov, Anton Raichuk, Rapha√´l Marinier, Damien Vincent, Marc Pollefeys, Timothy Lillicrap, Sylvain Gelly [(Savinov et al., 2018)]
7. Optimizing Agent Behavior over Long Time Scales by Transporting Value. Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico Carnevale, Arun Ahuja, Greg Wayne [(Hung et al., 2018)]
8. Uncertainty in Neural Networks: Bayesian Ensembling. Tim Pearce, Mohamed Zaki, Alexandra Brintrup, Nicolas Anastassacos, Andy Neely [(Pearce et al., 2018)]
9. The Laplacian in RL: Learning Representations with Efficient Approximations. Yifan Wu, George Tucker, Ofir Nachum [(Wu et al., 2018)]
10. How Does Batch Normalization Help Optimization? Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry [(Santurkar et al., 2018)]
11. Understanding disentangling in Œ≤-VAE. Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, Alexander Lerchner [(Burgess et al., 2018)]
12. Deep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun [(He et al., 2015)]
13. Understanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio [(Glorot & Bengio, 2010)]
14. Combined Reinforcement Learning via Abstract Representations. Vincent Fran√ßois-Lavet, Yoshua Bengio, Doina Precup, Joelle Pineau [(Fran√ßois-Lavet et al., 2018)]
15. MIT AGI: Building machines that see, learn, and think like people. Josh Tenenbaum [(Tenenbaum @ MIT AGI, 2018)]
16. Human-level performance in first-person multiplayer games with population-based deep reinforcement learning. Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel [(Jaderberg et al., 2018)]
17. Scalable agent alignment via reward modeling: a research direction. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg [(Leike et al., 2018)]
18. Curiosity-driven Exploration by Self-supervised Prediction. Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell [(Pathack et al., 2017)]
19. GAN Q-learning. Thang Doan, Bogdan Mazoure, Clare Lyle [(Doan et al., 2018)]
20. Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning. Noah Frazier-Logue, Stephen Jos√© Hanson [(Frazier-Logue & Hanson, 2018)]

My interpretation
-----------------

That's a chunky list in the classic style of [M. Brundage](https://twitter.com/Miles_Brundage) or his [mechanical dopleganger](https://twitter.com/BrundageBot). Here it is again with my interpretation of each paper and why I think they are really good:

### 1. Shattered Gradients

[(Balduzzi et al., 2017)] analyse the surprising performance of residual networks compared to plain feed forward nets, especially at high layer count. They show that feed forward gradients approach white noise exponentially with layers. On the other hand resnets gradients transition to white noise at sublinear rate with layers. They theorize that white noise gradients provide little information when training and design the Looks Linear initialization to alleviate the problem. They successfully train a 200 layer feed forward net in an empirical experiment, obtaining the same performance as a resnet on CIFAR-10.

Deeper networks have exponentially increasing representation power. While residual networks can be deep they are also much more constrained than plain old networks. This paper thus greatly increases the design space available.

### 2. Bayesian Action Decoder

[(Foerester et al., 2018)] describe a neat inductive bias for agents to learn what private information other collaborating agents have about the state. They condition a set of collaborating agents to deterministically choose policy based on the public information available. The agents then make their private observations and act according to the policy. Because the policy is based on the public state and known to all, other agents can then infer the private state based on the action taken. They successfully test it on Hanabi and get near optimal scores. Also Hanabi sounds like a fun game!

This method of inferring the private information of other agents is conceptually simple. It also avoids the intractable recurrence of modelling other agents' beliefs, and beliefs about beliefs...

### 3. Modular Networks

[(Kirsch et al., 2018)] implement a principled modular neural net that succeeds in specializing sub-networks to different tasks, or different parts of a sequential task as in text processing. Without using an entropy regularization term they avoid the problem of "module collapse", when a single module is selected for all tasks. Unlike mixture of experts models, where computation is still carried through for all experts, the modules sharply specialize for each task. The work shows a neat examples of a recurrent network using a specialised module for words following "the". Unfortunately they report poor generalization when testing on CIFAR-10 images.

To move from small scale, focused, experiments to extremely large, generalist, networks I believe we *need* modules if we ever want them to run on commodity hardware. Similarly to how the brain saves on energy and chemical budgets by using [10%](https://www.imdb.com/title/tt2872732/) at a time.

### 4. Attention Is All You Need

A transformative üêÜ work by the Google team [(Vaswani et al., 2017)], achieving state of the art results in language tasks by doubling down on attention and foregoing recurrence and convolutions. The main intuition is that attention can connect distant parts of the source sequence through a constant number of neural net steps, $O(1)$. In contrast recurrent nets need to keep distant inputs in memory for each step of the sequence, $O(n)$. At an intermediate level sit convolutional nets which need $O(log_k(n))$ layers for $k$ wide filters. The downside is that attention requires $O(n^2)$ compute, but this is trivially parallelizable.

I believe most problems can be solved by analysing a few influential factors in a sea of clutter, in this sense, attention is the appropriate inductive bias if only we could bypass the poor computational scaling.

### 5. Causal Influence in Multi-Agent RL

[(Jaques et al., 2018)] develop an intrinsic reward to encourage cooperation between independently trained agents. Unlike related works the agents don't rely on common training, they use no cheap communications channel, and do not share rewards. Instead each agent receives an intrinsic reward proportional to their empowerment on other agents, in less fancy words, how much they can influence the behaviour of others. Influence is measured by counterfactual reasoning in the learned model of other agents' behaviour, where each agents checks which of its actions can greatly change the actions of others. In their toy collaborative example an agent learns to act as a scout by spinning around when there's no high reward in sight, allowing others to keep collecting low rewards instead of exploring.

The paper presents empowerment as a proxy for coordination. Unlike other multi-agent works, both acting, rewards and training is independent for each agent.


*To be continued...*

<!-- References: -->

[(Balduzzi et al., 2017)]: https://arxiv.org/abs/1702.08591
[(Foerester et al., 2018)]: https://arxiv.org/abs/1811.01458
[(Kirsch et al., 2018)]: https://arxiv.org/abs/1811.05249
[(Vaswani et al., 2017)]: https://arxiv.org/abs/1706.03762
[(Jaques et al., 2018)]: https://arxiv.org/abs/1810.08647
[(Savinov et al., 2018)]: https://arxiv.org/abs/1810.02274
[(Hung et al., 2018)]: https://arxiv.org/abs/1810.06721
[(Pearce et al., 2018)]: https://arxiv.org/abs/1810.05546
[(Wu et al., 2018)]: https://arxiv.org/abs/1810.04586
[(Santurkar et al., 2018)]: https://arxiv.org/abs/1805.11604
[(Burgess et al., 2018)]: https://arxiv.org/abs/1804.03599
[(He et al., 2015)]: https://arxiv.org/abs/1512.03385
[(Glorot & Bengio, 2010)]: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
[(Fran√ßois-Lavet et al., 2018)]: https://arxiv.org/abs/1809.04506
[(Tenenbaum @ MIT AGI, 2018)]: https://www.youtube.com/watch?v=7ROelYvo8f0&list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4&index=2
[(Jaderberg et al., 2018)]: https://deepmind.com/blog/capture-the-flag/
[(Leike et al., 2018)]: https://arxiv.org/abs/1811.07871
[(Pathack et al., 2017)]: https://arxiv.org/abs/1705.05363
[(Doan et al., 2018)]: https://arxiv.org/abs/1805.04874
[(Frazier-Logue & Hanson, 2018)]: https://arxiv.org/abs/1808.03578


{% include mathjax.html %}