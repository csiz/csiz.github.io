---
title: Really Good Papers (part 1)
layout: post
---

I've started a challenge to read a machine learning paper a day. Now after a few of months and a lot of catching up to do I've gone through a couple hundred papers. These are my picks for the best ones I found:

(too many to list in the blurb, see [full post]({% post_url 2019-01-29-papers-1 %}))

<img src="/assets/images/paper-stack-1.jpg" width="360" alt="Stack of read papers."/>


<!--more-->

1. The Shattered Gradients Problem: If resnets are the answer, then what is the question? David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, Brian McWilliams [(Balduzzi et al., 2017)]
2. Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning. Jakob N. Foerster, Francis Song, Edward Hughes, Neil Burch, Iain Dunning, Shimon Whiteson, Matthew Botvinick, Michael Bowling [(Foerester et al., 2018)]
3. Modular Networks: Learning to Decompose Neural Computation. Louis Kirsch, Julius Kunze, David Barber [(Kirsch et al., 2018)]
4. Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin [(Vaswani et al., 2017)]
5. Intrinsic Social Motivation via Causal Influence in Multi-Agent RL. Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro A. Ortega, DJ Strouse, Joel Z. Leibo, Nando de Freitas [(Jaques et al., 2018)]
6. Episodic Curiosity through Reachability. Nikolay Savinov, Anton Raichuk, Raphaël Marinier, Damien Vincent, Marc Pollefeys, Timothy Lillicrap, Sylvain Gelly [(Savinov et al., 2018)]
7. Optimizing Agent Behavior over Long Time Scales by Transporting Value. Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico Carnevale, Arun Ahuja, Greg Wayne [(Hung et al., 2018)]
8. Uncertainty in Neural Networks: Bayesian Ensembling. Tim Pearce, Mohamed Zaki, Alexandra Brintrup, Nicolas Anastassacos, Andy Neely [(Pearce et al., 2018)]
9. The Laplacian in RL: Learning Representations with Efficient Approximations. Yifan Wu, George Tucker, Ofir Nachum [(Wu et al., 2018)]
10. How Does Batch Normalization Help Optimization? Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry [(Santurkar et al., 2018)]
11. Understanding disentangling in β-VAE. Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, Alexander Lerchner [(Burgess et al., 2018)]
12. Deep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun [(He et al., 2015)]
13. Understanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio [(Glorot & Bengio, 2010)]
14. Combined Reinforcement Learning via Abstract Representations. Vincent François-Lavet, Yoshua Bengio, Doina Precup, Joelle Pineau [(François-Lavet et al., 2018)]
15. MIT AGI: Building machines that see, learn, and think like people. Josh Tenenbaum [(Tenenbaum @ MIT AGI, 2018)]
16. Human-level performance in first-person multiplayer games with population-based deep reinforcement learning. Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel [(Jaderberg et al., 2018)]
17. Scalable agent alignment via reward modeling: a research direction. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg [(Leike et al., 2018)]
18. Curiosity-driven Exploration by Self-supervised Prediction. Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell [(Pathack et al., 2017)]
19. GAN Q-learning. Thang Doan, Bogdan Mazoure, Clare Lyle [(Doan et al., 2018)]
20. Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning. Noah Frazier-Logue, Stephen José Hanson [(Frazier-Logue & Hanson, 2018)]

<!-- References: -->

[(Balduzzi et al., 2017)]: https://arxiv.org/abs/1702.08591
[(Foerester et al., 2018)]: https://arxiv.org/abs/1811.01458
[(Kirsch et al., 2018)]: https://arxiv.org/abs/1811.05249
[(Vaswani et al., 2017)]: https://arxiv.org/abs/1706.03762
[(Jaques et al., 2018)]: https://arxiv.org/abs/1810.08647
[(Savinov et al., 2018)]: https://arxiv.org/abs/1810.02274
[(Hung et al., 2018)]: https://arxiv.org/abs/1810.06721
[(Pearce et al., 2018)]: https://arxiv.org/abs/1810.05546
[(Wu et al., 2018)]: https://arxiv.org/abs/1810.04586
[(Santurkar et al., 2018)]: https://arxiv.org/abs/1805.11604
[(Burgess et al., 2018)]: https://arxiv.org/abs/1804.03599
[(He et al., 2015)]: https://arxiv.org/abs/1512.03385
[(Glorot & Bengio, 2010)]: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
[(François-Lavet et al., 2018)]: https://arxiv.org/abs/1809.04506
[(Tenenbaum @ MIT AGI, 2018)]: https://www.youtube.com/watch?v=7ROelYvo8f0&list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4&index=2
[(Jaderberg et al., 2018)]: https://deepmind.com/blog/capture-the-flag/
[(Leike et al., 2018)]: https://arxiv.org/abs/1811.07871
[(Pathack et al., 2017)]: https://arxiv.org/abs/1705.05363
[(Doan et al., 2018)]: https://arxiv.org/abs/1805.04874
[(Frazier-Logue & Hanson, 2018)]: https://arxiv.org/abs/1808.03578
